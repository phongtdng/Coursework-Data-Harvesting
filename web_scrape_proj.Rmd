---
title: "Pink Tax Spain"
author: "Phong Duong, Nienke Visscher"
date: "2024-02-15"
output: html_document
editor_options: 
  chunk_output_type: inline
---

## Library

```{r}
library(xml2)
library(tidyverse)
library(RSelenium)
library(glue)
library(magrittr)
library(httr)
library(sf)
library(rnaturalearth)
library(scrapex)
library(lubridate)
library(dplyr)
library(tidyr)
```

## Mercadona

```{r}

#Code for attaching JAVA
#Sys.getenv("JAVA_HOME")
#Sys.setenv(JAVA_HOME = "C:/Program Files/Common Files/Oracle/Java/javapath/java.exe")
#Sys.getenv("PATH")
#install.packages("RSelenium", dependencies = TRUE)

```

### Setting up RSelenium

```{r}
set_config(
  user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36; Nienke Visscher / nienvis@gmail.com")
)

#Opening hidden browser
remDr <- rsDriver(port = 4464L, browser = "firefox",  verbose=F, chromever = NULL)


merc <-"https://tienda.mercadona.es/categories"

browseURL(prep_browser(merc))
 
remDr$client$navigate(merc)


```

### Obtaining URLs

#### Clicking through opening page

```{r}
driver <- remDr$client

#Note following only when having newly initiated the port
#Accepting cookies
driver$findElement(value = "(//button[@class = 'ui-button ui-button--small ui-button--tertiary ui-button--positive'])[1]")$clickElement()

#Click on text box
driver$findElement(value = "(//input[@class = 'ym-hide-content'])[1]")$clickElement()

#Define findElement of the textbox
codigo <-driver$findElement(value = "(//input[@class = 'ym-hide-content'])[1]")

#Fill in the text box
codigo$sendKeysToElement(list("28014"))

#Click 
driver$findElement(value = "(//button[@class = 'button button-primary button-big'])[1]")$clickElement()

```

#### Obtain the URLs per category

```{r}

#Get URLs 'Cuidado del cabello'

#Create an empty list to store the URLs 
url <- list()

#Navigate to the button 'Cuidado del cabello'
driver$findElement(value = "(//label[@class='subhead1-r' and contains(text(), 'Cuidado del cabello')])[1]")$clickElement()

#Use system sleep to avoid code running faster than the internet
Sys.sleep(1)

#Store the first URL
url[1] <-driver$getCurrentUrl()

#Create a loop to click on the button at the end of the page to naviagte to the next category and save the url
for (i in (2:4)){
  
  driver$findElement(value = "(//button[@class = 'ui-button ui-button--big ui-button--secondary ui-button--positive category-detail__next-subcategory'])[1]")$clickElement()
  
Sys.sleep(1)
  
  url[i] <-driver$getCurrentUrl()
  
  
}

#Get URLs Cuidado facial y corporal
  
driver$findElement(value = "(//label[@class='subhead1-r' and contains(text(), 'Cuidado facial y corporal')])[1]")$clickElement()

Sys.sleep(1)

url[5] <-driver$getCurrentUrl()



for (i in (6:15)){
  
  
  driver$findElement(value = "(//button[@class = 'ui-button ui-button--big ui-button--secondary ui-button--positive category-detail__next-subcategory'])[1]")$clickElement()
  
  Sys.sleep(1)
  
  url[i] <-driver$getCurrentUrl()
  
}

#Unlist the URL variable
url <-unlist(url)


```

### Scraping and cleaning data

```{r}

#Create function
cat_grabber <- function(url){
  
#Navigate to one of the URLs stored in the variable URL
driver$navigate(url)
  
#Use system sleep to avoid code running faster than the internet
Sys.sleep(1)

#Get the page source code and read the html
cat_html <-driver$getPageSource()[[1]] |>
  read_html()

#Collect data on products
cat_info <- cat_html|> 
    xml_find_all("//*[@id='root']//button/div[2]") |>
    xml_text()

#Collect the sub category names
subcat_name <- cat_html |> 
   xml_find_all("//h1[@class='category-detail__title title1-b']") |>
   xml_text()

#Clean and separate data using regex
cat_clean<-cat_info|> 
  str_replace_all("\\€ |/|\\.|!", "") |>
  str_replace_all("\\€\\d+,\\d+", "") |>
  str_replace_all("(?<=\\d\\s)ml", "/ml/") |> 
  str_replace_all("(?<=\\d\\s)ud", "/ud/") |> 
  str_replace_all("(?<=\\d\\s{0,2})ud", "/ud/") |> 
  str_replace_all("(?<=\\d\\s)L", "/L/") |> 
  str_replace_all("(?<=\\d\\s)tarro", "") |> 
  str_replace_all("(?<=\\d\\s)g\\s", "/g/") |> 
  str_replace_all("(?<=\\d\\s)g", "/g/") |> 
  str_replace_all("(?<=\\d\\s)sobres", "/sobres/") |> 
  str_replace_all("(?<=\\d\\s)bandas", "/bandas/") |> 
  str_replace_all("(?<=sensibleCaja\\s)1", "/") |> 
  str_replace_all("(?<=DovePaquete\\s)4", "/") |> 
  str_replace_all("ud.(?=\\s*\\()", "") |> 
  str_replace_all("\\s(?=\\d+\\s*/)", "/") |> 
  str_replace_all("(?<=[[:lower:]])(?=[[:digit:]])", "/") |> 
  str_replace_all("\\(|\\)", "") |> 
  str_replace_all("\\.", "")|> 
  strsplit("/")



#Only keep the variables with 5 or less strings
cat_clean<- cat_clean[lengths(cat_clean) == 5]

#Create a dataframe
data.frame(
  name = sapply(cat_clean, `[`, 1),
  quantity = sapply(cat_clean, `[`, 2),
  quantity_unit = sapply(cat_clean, `[`, 3),
  price = sapply(cat_clean, `[`, 4),
  ud =  sapply(cat_clean, `[`, 5),
  subcat =  sapply(subcat_name, `[`, 1),
  stringsAsFactors = FALSE
)


}

cat_grabber(url)

#Loop thhe function cat_grabber over all URLs
final <- map_dfr(url, cat_grabber) 

```

### Scrape and add main categories to data frame

```{r}

driver$navigate(url[1])
  
Sys.sleep(1)

cat_html <-driver$getPageSource()[[1]] |>
  read_html() 

cat <- cat_html |> 
   xml_find_all("//label[@class='subhead1-r']") |>
   xml_text()


final <- final |> 
  mutate(cat_name = ifelse((subcat=="Acondicionador y mascarilla"|
                              subcat== "Champú"|
                              subcat== "Coloración cabello"|
                              subcat== "Fijación cabello" ),cat[14], cat[15]))

```

```{r}
#Deslect column ud bacause this column does not contain variation
final <- final |> 
  dplyr::select(-ud)
```

### Brands

```{r}

brands <- c("Deliplus", "Elvive", "Pantene", "O'lysee", 
            "H&S", "Ultrex", "Garnier", "L'Oréal", 
            "Schwarzkopf", "Nelly", "Giorgi", "Elnett", 
            "Fructis", "Syoss", "Gillette", 
            "Nivea", "Axe", "Atlantia", "Bosque Verde", 
            "Montagne Jeunesse", "Viseger Pharma", 
            "My Moment", "Khanya", "Veet", "Wilkinson", 
            "Sanex", "Byly", "Tulipán Negro", "Dove", 
            "Deonat", "Rexona", "Heno de Pravia", "960", 
            "La Toja", "Magno", "Lactovit", "Colgate", 
            "Oral-B", "Sensodyne", "Signal", "Parodontax", 
            "Benfix", "Listerine", "Polident", "evax", 
            "Ausonia", "Carefree", "Tampax", "Tena",
            "Sense", "Classic fresh", "Rose Nude", 
            "Elección", "Prêt à Porter", "Como Tú", 
            "Como Tú", "Psicodelic", "Complicity", "Vuela", 
            "Adidas", "Soplo", "Capítulo Floral", "My soul", 
            "Blue Shine", "Verissime", "Flor de Mayo", 
            "Monogotas", "Capítulo", "Extrait", 
            "Mr. Wonderful", "Uahuu", "TokiDoki", "Ikiru",
            "Chupachups", "Rose Nude", "Rouge seduction",
            "Snoopy", "Muy mío", "Impacto", "Afán", 
            "Springfield", "Gesto", "Sesgo", "Brummel", 
            "Antonio Banderas", "Misty Wood", "Capítulo Marino", "Ego", 
            "My Soul", "Jacq's", "My Soul", "Selk",
            " Crossmen", "Gesto", "Disney", "Marvel", 
            "Naruto", "Peppa Pig", "Sonic de Hedgehog", "Colour up!", 
            "HoneyBotella")


brands_or <- paste(brands, collapse = "|")

final$brand <- str_extract_all(final$name, regex(brands_or, ignore_case = TRUE))

```

### Average prices

```{r}
units <- c("L", "ml", "ud", "sobres", "bandas", "g")

mercadona <- final |> 
  filter(quantity_unit %in% units) |> 
  filter(str_detect(price, "[0-9]")) |> 
  mutate(price = str_replace_all(price, ",", ".")) |> 
  mutate(quantity = as.numeric(quantity),
         price = as.numeric(price)) |> 
  mutate(avg_price = case_when(
    str_detect(quantity_unit, "g|ml") ~ (price/quantity)*100,
    str_detect(quantity_unit, "L") ~ (price/quantity)/100,
    str_detect(quantity_unit, "ud") ~ price /quantity
    )) |> 
   mutate(avg_price = round(avg_price, 2))

```

### Gender product

```{r}
#Vector of words indicating gender
gender <-c("Men", "Women", "Man", "Woman", "Hombre", "Mujer", "Hombres", "Mujeres", "Him", "Her", "Niña", "Niñas", "Homme", "Femme", "Male", "Female", "Males", "Females", "Venus") 

#Collapsing the gender vector to 
gender_or <- paste(gender, collapse = "|")

mercadona <-mercadona |> 
  mutate(combined =  paste(mercadona$name, mercadona$subcat, sep = " ")) |> 
  mutate(gender_name =  str_extract(combined, regex(paste0("\\b(", gender_or, ")\\b"), ignore_case = TRUE)) ) |> 
  select(-combined)

# Convert NA to 'Non-Male'

mercadona$gender_name[is.na(mercadona$gender_name)] <- "Non-Male"


#Create Male and Female vectors
male <- c("Men", "Man", "Hombre", "Hombres", "Him", "Homme", "Male",  "Males") 

male <- c(male, tolower(male))

female <- c("Women", "Woman", "Mujer", "Mujeres", "Her", "Niña", "Niñas", "Femme", "Female", "Females", "Venus") 

female <- c(female, tolower(female))

#Assign gender to products (separate column for female)
mercadona$gender <- case_when(
  mercadona$gender_name %in% male | mercadona$subcat %in% male ~ "Male",
  mercadona$gender_name %in% female|mercadona$subcat %in% female ~ "Female",
  mercadona$gender_name == "Non-Male"~ "Non-Male")

#Assign gender to products (separate column for female)
mercadona$gender_2 <- case_when(
  mercadona$gender == "Male" ~ "Male",
  mercadona$gender == "Female" ~ "Non-Male",
  mercadona$gender == "Non-Male"~ "Non-Male")


```

## Mean comparisons

### Categories

```{r}

#Compare all male and non-male products in the category "Cuidado del cabello" in ml, however impossible because no male products
cat_1_m <- mercadona |> 
  filter(cat_name == "Cuidado del cabello" & quantity_unit == "ml")

prices_male <- cat_1_m$avg_price[cat_1_m$gender_2 == "Male"]
prices_non_male <- cat_1_m$avg_price[cat_1_m$gender_2 == "Non-Male"]

# Perform independent samples t-test
t_test_result <- t.test(prices_male, prices_non_male, alternative = "greater")
t_test_result

#Compare all male and non-male products in the category "Cuidado del cabello" in ud
cat_1_m_ud <- mercadona |> 
  filter(cat_name == "Cuidado del cabello" & quantity_unit == "ud")

prices_male <- cat_1_m_ud$avg_price[cat_1_m_ud$gender_2 == "Male"]
prices_non_male <- cat_1_m_ud$avg_price[cat_1_m_ud$gender_2 == "Non-Male"]

# Perform independent samples t-test
t_test_result <- t.test(prices_male, prices_non_male, alternative = "greater")
t_test_result


#Compare all male and non-male products in the category "Cuidado facial y corporal" in ml, however impossible because no male products
cat_2_m <- mercadona |> 
  filter(cat_name == "Cuidado facial y corporal" & quantity_unit == "ml")

prices_male <- cat_2_m$avg_price[cat_2_m$gender_2 == "Male"]
prices_non_male <- cat_2_m$avg_price[cat_2_m$gender_2 == "Non-Male"]

# Perform independent samples t-test
t_test_result <- t.test(prices_male, prices_non_male, alternative = "less")
t_test_result


#Compare all male and non-male products in the category "Cuidado facial y corporal" in ud, however impossible because no male products

cat_2_m_ud <- mercadona |> 
  filter(cat_name == "Cuidado facial y corporal" & quantity_unit == "ud")

prices_male <- cat_2_m_ud$avg_price[cat_2_m_ud$gender_2 == "Male"]
prices_non_male <- cat_2_m_ud$avg_price[cat_2_m_ud$gender_2 == "Non-Male"]

# Perform independent samples t-test
t_test_result <- t.test(prices_male, prices_non_male, alternative = "less")
t_test_result

library(broom)

tidy(t.test(prices_male, prices_non_male, alternative = "less"))

```

```{r}
library(dplyr)
library(purrr)

cat_f <- c("Cuidado del cabello")
quan_f <- c("ud")


t_test_fun <- function(cat_f, quan_f) {
  cat_d <- mercadona %>%
    filter(cat_name == cat_f & quantity_unit == quan_f)
  
  prices_male <- cat_d$avg_price[cat_d$gender_2 == "Male"]
  prices_non_male <- cat_d$avg_price[cat_d$gender_2 == "Non-Male"]
  
  t_test_result <- NULL
  
  tryCatch({
    t_test_result <- tidy(t.test(prices_male, prices_non_male))
  }, error = function(e) {NA
    
    print(e)
  })
}


t_test_fun(cat_f, quan_f)

# Define combinations of cat_f and quan_f
cat_f <- c("Cuidado del cabello", "Cuidado del cabello","Cuidado facial y corporal","Cuidado facial y corporal")
quan_f <- c("ml", "ud", "ml", "ud")

# Perform t-tests for all combinations
t_test_results <- map2_dfr(cat_f, quan_f, t_test_fun)

# Print or access the t-test results
print(t_test_results)


```

### sub categories

```{r}

cat_1_1_m <- mercadona |> 
  filter(subcat == "Cuidado del cabello" & quantity_unit == "ml")

prices_male <- cat_1_m_ud$avg_price[cat_1_m_ud$gender_2 == "Male"]
prices_non_male <- cat_1_m_ud$avg_price[cat_1_m_ud$gender_2 == "Non-Male"]

# Perform independent samples t-test
t_test_result <- t.test(prices_male, prices_non_male)
t_test_result
```

## Clarel

For Clarel website, there is a lot of content hidden through Javascript so we will use Selenium to scrape all the interested content. The setup on my computer was initially troublesome as I wanted to use Chrome as browser. There were a few fix suggestions and the only one that worked for me was to delete the "LISCENSE.chromever" file for the corresponding. However, there was no clear explanation to this fix and the potential issues with deleting the file so we will use Firefox browser for selenium instead.

### Data Collection

```{r}
driver <- rsDriver(browser = "firefox",
                   chromever = NULL,
                   verbose = F)

remDr <- driver[["client"]]
```

We first initialize the Selenium server and create a remote driver session. We're saving the client object to a variable for quicker access later on.

#### Main Webpage

```{r}
clarel_url <- "https://www.clarel.es/es/"

remDr$navigate(clarel_url)

Sys.sleep(5)

#Reject cookies
remDr$findElement(using = "xpath", "//button[contains(@class, 'info_cookie-consent-reject-button')]")$clickElement()
```

Once we navigate to the home page of Clarel, there is a pop up for cookies so we will need to either accept or reject the cookies. We leave the system to pause for 5 seconds after navigating to the home page so the browser has sufficient time to load everything. If reject cookies line code runs into error, most likely it is because the page has not fully loaded and the pop up has not appeared yet therefore the button is not found. It is necessary to run again the reject cookies code. If it still runs into error, it is likely that the elements in the web page have changed their name.

#### Scrape Hombre Category

We will start by scraping one category before streamline the scraping process for multiple categories. The structure of the pages of all categories are quite similar so we should not run into many troubles later on if we can scrape one category properly. In this project, I chose to start with "Hombre" category.

```{r}
#helper function to get all subcategories of main category input
get_subcat2_url <- function(category){
  #get all the nodes containing category keyword
  all_nodes <- remDr$getPageSource()[[1]] %>% 
    read_html() %>% 
    xml_find_all(glue("//a[contains(@href,'/{category}/')]")) %>% 
    xml_attr("href") %>% 
    .[str_detect(.,"https",negate = TRUE)] %>% 
    unique(.)
  
  #get subcategories and sub-subcateogry(product type) 
  sub_cat <- all_nodes[str_detect(all_nodes, glue("es/{category}/\\w+(-\\w+)?$"))]
  product_type <- all_nodes[str_detect(all_nodes, glue("{category}/\\w+(-\\w+)?/\\w+"))]
  
  #identify subcategories with no product type to include in list to scrape
  no_product_type <- c()
  for (i in sub_cat) {
    if(!any(str_detect(product_type, i))) {
      no_product_type <- c(no_product_type,i)
    }
  }
  
  #all urls to subcategories and sub-subcategories of the main categories
  all_urls <- c(no_product_type, product_type) %>% 
    paste0("https://www.clarel.es",.)
  
  return(all_urls)
}

#test run
sub_cat_2_url <- get_subcat2_url("hombre")
sub_cat_2_url
```

In each main category of products on Clarel, there are also sub-categories and in each of the sub-category sometimes there are sub-section (I will refer to them as product type) the details the product type as well. We would like to get all of this information so later we have more resources to work with for the analysis in case that we need it.

To do this, first I created a helper function to grab all the URLs to the sub-categories and product types. Another option would have been to navigate to each sub-category and grab the product types but it is more time consuming as we would need to go to multiple product type pages of multiple sub-category pages and the loading time would amass quickly. Therefore, I tried to identify all the URLs from the home page but the trade-off is some cleaning in the process.

The logic of this function goes like this:

-   The input (parameter) is the main category that we are interested in.

-   The function then finds all the anchor links containing the category, extracts those links, cleans irrelevant links and duplicated links, then saves them into a variable.

-   The function continues to extract sub-category links and product type links. The product types are subsection of the sub-category, therefore, we need to exclude them from the URL list to avoid scraping duplicated items.

-   The cleaning of these links are done by detecting if the URL list of product type contains the sub-category. (For example, "afeitado" sub-category URL contains "es/hombre/afeitado". A product type of this sub-category is "despues afeitado" and its URL would be "es/hombre/afeitado/despeues-afeitado". The URL of "despues afeitado" contains URL of afeitado so we can base on that and exclude "afeitado" from the list)

-   The function returns a vector of product type URLs and sub-categories with no product types URLs

Using this function, we can obtain all the necessary URLs to scrape all the products of the main category divided by sub-categories and product types. Now we will move on to scrape the products of each URL.

```{r}
#Load more items helper function
load_all_items <- function() {
  Sys.sleep(3.5)
  tryCatch(
    {
      suppressMessages({
      load_more_button <-remDr$findElement(using = "xpath", 
                                          "//button[contains(@class, 'load-more-items-button')]")
      while(load_more_button$isElementDisplayed()[[1]]) {
      load_more_button$clickElement()
      Sys.sleep(3.5)
      load_more_button <-remDr$findElement(using = "xpath", 
                                           "//button[contains(@class, 'load-more-items-button')]")
      }
      })
    },
    error = function(e){})
}

#test run
# load_all_items()
```

Navigating to the URL of the sub-categories and product types gives us a list of products. However, pages have a lot of products and not all are shown on the page. These products are hidden after the "Ver más productos" ("See more products") and we need to repeatedly click on the button until all the products are shown.

Above, I created a function to click the button until the button is gone, meaning that all products have been loaded. There are pauses between each click to wait for the products to load. Since we don't know how many times we have to click the button, we use a `while` loop to click the button until there is no more button to click. When there is no more button, we would get an error if we tried to find the button so we use a `tryCatch()` function to catch and ignore this error.

With this helper function to load all the products, we now can scale and go to all URL, load all the products, and scrape them.

```{r}
#Get all product info helper function
get_product_info <- function(url) {
  remDr$navigate(url)
  
  load_all_items()
  
  paths <- read_html(remDr$getPageSource()[[1]]) %>% 
  xml_find_all("//div[contains(@class, 'box_product-info')]")
  
  get_info <- function(path) {
  brand <- path %>% 
    xml_find_all("div[contains(@class, 'product-featuredbrands')]") %>%
    xml_text()
  
  product_name <- path %>% 
    xml_find_all("a") %>% 
    xml_text()
  
  price <- path %>% 
    xml_find_all("div[contains(@class, 'product-price')]/span[contains(@class,'final-price')]") %>% 
    xml_text()
  
  data.frame(
    brand = brand,
    product_name = product_name,
    price = price
  )
  }
  
  #Get categories & subcategories to add to the dataframe
  cat <- remDr$getPageSource()[[1]] %>% 
    read_html() %>% 
    xml_find_all("//span[@class='subcategory']") %>% 
    xml_text() %>% 
    paste(collapse = "-")
  
  df <- map_dfr(paths, get_info) %>% mutate(full_cat = cat)

  
  return(df)
}

#test run
hombre_products <- map_dfr(sub_cat_2_url, get_product_info)

hombre_products
```

Now, we need to get the information of the products. We will similarly create a helper function to streamline the process. The function I created take a URL for input, navigates to that URL, loads all the products using the function created previously, then it gets all the paths (nodes) of the `div` tags containing information about the products. After this, I nested a function to grab the product information from the `div` tags, which at the end returns the data frame of the product information, including brand name, product name, and price. Since categories, sub-categories, and product types are in other elements of the page, I grabbed them separately and append to the data frame to finally create a data frame with brand, product name, price, and full category columns consisting of main category, sub-category, and product type (if any) joined together by "-" (e.g., "Hombre-Afeitado-Después Afeitado").

#### Scrape Multiple Categories

```{r}
#all main categories to be scraped
imp_cat <- c("hombre", "higiene", "cabello", "perfumes")

#helper function to scrape data on large scale
scrape_all <- function(category) {
  remDr$navigate(clarel_url)
  
  subcat2 <- get_subcat2_url(category)
  
  df <- map_dfr(subcat2, get_product_info) 
  
  return(df)
}

all_products_clarel <- map_dfr(imp_cat ,scrape_all)

#write into csv to avoid scraping every time
write_csv(all_products_clarel, "datasets/clarel.csv")
```

With the helper functions created before, we have all the tools needed to scrape products from Clarel web page on a large scale (multiple categories). We create a vector with all the interested categories and a function to automate the scraping. This function navigates to the home page of Clarel, then extracts all URLs of sub-categories and product types of the main category input, and finally gets all the product information using the function from the last section to produce a final data frame. We use `map_dfr` to apply the function on the vector of interested categories and create a comprehensive data frame.

At the time of writing this project, it took around 8 minutes to scrape the 4 categories hombre, higiene, cabello, and perfumes. I saved the scraped data into a csv file so for the analysis we would not have to repeat the scrape every time. Furthermore, if there are any changes to the web page and the functions no longer work, the analysis is still reproducible with the csv file.

### Data Cleaning

```{r}
#loading the data frame from the file created after scraping
all_products_clarel <- read_csv("datasets/clarel.csv")
```

```{r}
#helper function to id gender & kid of products
gender_id <- function(str){
  male <- c("men", "man", "hombre", "hombres", "him", "homme", "male",  "males", "masculino") 

  female <- c("women", "woman", "mujer", "mujeres", "her", "femme", "female", "females", "venus", "femenina") 

  kids <- c("kid", "kids", "niños", "niño", "niñas", "niña", "infantil", "infantiles")
  
  str <- tolower(str)
  
  if(str_detect(str, paste0("\\b(", kids, ")\\b", collapse = "|"))) {
    return("Kid")
  } else if(str_detect(str, paste0("\\b(", female, ")\\b", collapse = "|"))) {
    return("Female")
  } else if(str_detect(str, paste0("\\b(", male, ")\\b", collapse = "|"))) {
    return("Male")
  } else {return(NA)}
  
}
```

It is extremely tricky to identify which products are aimed at men or women (especially women) because most products do not include this information. They are more easily identified through the pictures and packaging choice (such as pink color and with a woman figure for women products) but since we work with only text we are very limited to the ability to identify to which gender the product is geared toward. Some products are in "Hombre" category so they are easily classified for men but the other categories do not clearly indicate gender. We will try to detect this information using keywords in the product names and sub-categories.

Above, we created some lists of words that could indicate whether the products are for men or women or kids. We also try to identify kids products because later on we will discard them since it is not the scope of this project. We then put those lists into a function that takes a string as input and detects whether in the string there is any word from the list, then categorizes the string accordingly.

```{r}
cleaned_clarel <- all_products_clarel %>% 
  as_tibble() %>% 
  #Remove duplicated products
  distinct(product_name, .keep_all = TRUE) %>% 
  #Mutate to later split into cateogries and subcategories
  mutate(
    full_cat = str_replace(full_cat, "-", "/")
  ) %>% 
  #Split category into sub category and product type
  separate_wider_delim(full_cat, delim = "/", names = c("category", "sub_category")) %>% 
  #Extract quantity with unit
  mutate(qty = str_extract(tolower(product_name), "\\d+ ?ml|\\d+ ?g|\\d+ ?ud(s)?|\\d+(,\\d+)? ?l") ,.after = product_name) %>% 
  #NA rows in quantity means product is sold in 1 unit
  mutate(qty = replace_na(qty, "1")) %>% 
  #Extract unit of quantity
  mutate(
    qty_unit = ifelse(qty == 1, "ud", str_extract(qty, "ml|l|ud|g")),
    price = parse_number(str_replace(price, "€", ""), locale = locale(decimal_mark = ",")),
    qty = parse_number(qty,locale = locale(decimal_mark = ","))) %>% 
  #Calculate average price
  mutate(
    avg_price = case_when(
      str_detect(qty_unit, "ml|g") ~ price * 100 / qty, #price per 100ml/g
      qty_unit == "ud" ~ price / qty, #price per unit
      qty_unit == "l" ~ price *0.1 / qty, #price per 100ml for Litters
      TRUE ~ price 
    )
  ) %>% 
  #Round average price and identify targeted gender of product
  mutate(avg_price = round(avg_price, 2),
         gender = case_when(
           (category == "Hombre" | str_detect(sub_category, "Hombre")) ~ "Male",
           TRUE ~ mapply(gender_id, paste(product_name, sub_category))
           )
         ) %>% 
  #Replace non gender specific product with non-male
  mutate(gender = replace_na(gender, "Non-Male"),
         #gender_2 to compare male vs non-male targeted products
         gender_2 = ifelse(gender == "Male", "Male", "Non-Male")) %>% 
  #Remove products for kids
  filter(gender != "Kid") %>% 
  #Reorganise column names to join with other datasets
  select(product_name, qty, qty_unit, price, sub_category, category, avg_price, gender, gender_2)
```

As for most web scraping projects, the scraped data is usually messy and needs diligent cleaning so it is appropriate for analysis. The same goes for the data scraped here previously.

First we remove the duplicated products since products in some categories overlap and therefore appear multiple times in the data set. We then split the full category into main category and sub-category (which also includes product types). We first replace the first "-" of the full category collumn with another symbol so we can split into 2 columns based on that symbol.

Next, we extract the quantity from the product name. The product name contains various information from the product to brand name, quantity, and more. We extract the quantity using Regex and look for numbers followed by quantity symbol (ml, L, g, u/ud/uds). If the product name does not contain any of these, it is most possible that the product is sold as one unit whole. Therefore, after extracting products with specific quantities, we impute the missing rows with 1 unit.

We then extract the unit of the quantity then calculate the average price based on the unit. For milliliter (ml), gram (g), and litter (l) the average price is per 100 ml or g. For products that are sold in more than 1 unit the average price is calculated as per unit. Lastly, they are rounded at 2 decimal places.

Finally, we detect the gender for which the product aims. First, all products in category "Hombre" are named "Male" and then we use the gender identifying function from before to detect if the product name or product sub-category/product type indicates that it is for men or women or kids. Since few products are explicitly described as for women, we make another label called "Non-Male" to compare specifically male-targeted products versus non-male-targeted products. The gender column include 4 different labels (Male, Female, Kid, and Non-Male). The gender_2 column consists of only 2 (Male and Non-Male). These are made for the purpose of analysis.

## Data Joining

```{r}
#Modify mercadona dataframe to join
mercadona_cleaned <- mercadona %>% select(-brand, -gender_name) 

#Match column names
colnames(cleaned_clarel) <- colnames(mercadona_cleaned)

#helper function
match_cat <- function(str) {
  str <- tolower(str)
  
  cat <- case_when(
    str_detect(str, "afeitado") ~ "Afeitado",
    str_detect(str, "champú") ~ "Champú",
    str_detect(str, "acondicionador|mascarilla") ~ "Acondicionador y Mascarilla",
    str_detect(str, "coloración|tinte") ~ "Coloración",
    str_detect(str, "fija") ~ "Fijación cabello",
    str_detect(str, "corporal") ~ "Cuidado corporal",
    str_detect(str, "facial") ~ "Cuidado facial",
    str_detect(str, "depilación") ~ "Depilación",
    str_detect(str, "desodorante") ~ "Desodorante",
    str_detect(str, "gel") ~ "Gel y Jabón",
    str_detect(str, "higiene bucal") ~ "Higiene bucal",
    str_detect(str, "higiene íntima") ~ "Higiene íntima",
    str_detect(str, "manicura|pedicura") ~ "Manicura y Pedicura",
    str_detect(str, "perfume") ~ "Perfume",
    str_detect(str, "solar") ~ "Protector solar y aftersun"
  )
  
  return(cat)
}

#Join 2 dataset together
big_df <- bind_rows(mercadona_cleaned, cleaned_clarel) %>% 
  #sub-category matching
  mutate(match_subcat = match_cat(subcat), .after = subcat) %>% 
  #match by product name if sub-cat doesn't match
  mutate(match_subcat = ifelse(is.na(match_subcat), match_cat(name), match_subcat)) %>% 
  #other subcats that have no match for either sub-cat and product name
  mutate(match_subcat = replace_na(match_subcat, "Other"))
```
