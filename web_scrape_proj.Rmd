---
title: "Scraping data on personal health products from Mercadona and Clarel to determine price differences between male and non-male marketed products"
author: "Phong Duong, Rianne Nienke Visscher"
date: "2024-02-15"
output: html_document
editor_options: 
  chunk_output_type: inline
---

## Library

```{r}
library(xml2)
library(tidyverse)
library(RSelenium)
library(magrittr) #? do we use this, also insf package
library(httr)
library(sf) #Do we use this
library(ROSE)
library(broom)
library(glue)
library(dplyr)
```

## Read Me

1.  Title: Scraping data on personal health products from Mercadona and Clarel to determine price differences between male and non-male marketed products

2.  Project description

    > This assignment aims to investigate price differences in personal care products in Spain. More specifically, are male personal care products priced differently compared to non-male products? To obtain the data needed for this analysis, data on personal health care products is scraped from the website of Mercadona and Clarel, a large Spanish supermarket and drugstore chain respectively. Then, mean comparisons are performed to determine any price differences between male and non-male products for different types of personal health products.

3.  Table of contents

    1.  Introduction
    2.  Libraries
    3.  Mercadona
        1.  Setting up RSelenium
        2.  Obtaining URLs
        3.  Scraping and cleaning data
        4.  Extracting brand names
        5.  Standardising prices
        6.  Assigning marketed gender to product
    4.  Clarel
        1.  Data collection
            1.  Main webpage
            2.  Scrape Hombre Category
            3.  Scrape Multiple Categories
        2.  Data cleaning
    5.  Data joining
    6.  Data balancing
    7.  Mean comparisons
    8.  Discussion and conclusion

4.  Technologies used

    > The coding language used to create this project is R.

5.  Requirements

    > To run the code in this project one needs to have installed the follwoing packages in R:
    >
    > -   [Tidyverse](https://cran.r-project.org/web/packages/tidyverse/index.html): Ecosystem of Packages designed for data science in R (glue, purrr)
    >
    > -   [xml2:](https://cran.r-project.org/web/packages/xml2/index.html) R package to handle xml files
    >
    > -   [RSelenium](https://cran.r-project.org/web/packages/RSelenium/index.html): R package providing bindings for the 'Selenium 2.0 WebDriver'. This package allows navigating a website using a driver immitating a regular user. Trough this navigation, one is able to collect html code from websites which would otherwise be hidden.
    >
    > -   [httr:](https://cran.r-project.org/web/packages/httr/index.html) Package including tools for working with URLs and HTTP
    >
    > -   [ROSE:](https://cran.r-project.org/web/packages/ROSE/index.html) *Random Over-Sampling Examples.* This package contains tools to apply a combination of under- and over-sampling techniques to balance unbalanced data sets
    >
    > -   [Broom:](https://cran.r-project.org/web/packages/broom/index.html) Package to convert statistical sbjects into tidy tibbles
    >
    > All packages used in this assignment can be downloaded into R using the function install.packages(). For example:
    >
    > ```{r}
    > #| eval: false
    >     >
    > install.packages("tidyverse")
    > ```

    > To use the package RSelenium, JAVA needs to be downloaded on the used device and correctly attached to the R. This can be done using the following code
    >
    > ```{r}
    > #| eval: false
    >     >
    > Sys.getenv("JAVA_HOME")
    > Sys.setenv(JAVA_HOME = "path/Oracle/Java/javapath/java.exe")
    > Sys.getenv("PATH")
    > install.packages("RSelenium", dependencies = TRUE)
    >     >
    > ```

    > To scrape websites ethically, one needs to set a configuration to indicate who is scraping the website and allow the owners of the website to contact the scraper in case the scraper is causing any issues to the website. Therefore, before running the code, one needs to fill in the user agent in the following piece of code. One can obtain the user agent by simply asking the google search engine 'What is my user agent?'
    >
    > ```{r}
    > #| eval: false
    >     >
    > set_config(
    >   user_agent("")
    > )
    > ```
    >
    > Note that when setting the rsdriver, one might needs to make some adjustments to the code. If the port is occupied, simply insert a different number and change the browser to the browser installed in the device used to run the code. In the developers case of this project, the browser 'Chrome' did not function well. Therefore, consider [downloading the browser Firefox](https://www.mozilla.org/en-US/firefox/new/) in case Chrome does not perform well.
    >
    > ```{r}
    > remDr <- rsDriver(port = 4467L, browser = "firefox",  verbose=F, chromever = NULL)
    > ```

## Introduction

When one strolls trough the isles with personal care products in a drug store or the supermarket, one will notice that products are clearly targeted at men or women. One will also notice that tehd majority of the isle is coloured pink, as the majority of personal care products are specifically marketed towards women. However, something one might not directly remarks is that these pink products are higher priced compared to the blue male targeted products. This overpricing of female products is known as the 'Pink tax' and is prevelent among all sorts of products ranging from clothing to toys (Economic Committee United States Congress, 2016). Guittar et al. (2022) researched this *pink tax* of personal health products in the US and find that women tend to pay more for products such as deoderants and lotions, whereas men tend to pay more for shaving gels. This assignment aims to investigate price differences in personal care products in Spain. More specifically, are male personal care products priced differently compared to non-male products? To obtain the data needed for this analysis, data on personal health care products is scraped from the website of Mercadona and Clarel, a large Spanish supermarket and drugstore chain respectively. Then, mean comparisons are performed to determine any price differences between male and non-male products for different types of personal health products. Unfortunately, the structure of the obained data does not allow to investigate price differences between male and female marketed products as the identification of the gender marketed of these products is done using the product or category name which contain information on the gender when it involves a male marketed product but not when it involves a female marketed products. Future research can utilise image detection methods to indicate the gender of the products which is out of the scope of this assignment.

This assignment is structured as follows: first the data on the personal health product from the Mercadona and the Clarel website are scraped and cleaned and added to a data frame. Then, the data is balanced after which t-test will be utilised to examine the price differences.

## Mercadona

### Setting up RSelenium

First, the user agent is being set to provide the website that is being scraped with the information of the scraper. This is done to allow the webmaster to contact the scraper in case she is causing any problems for the functioning of the website. Then the background browser is being opened using the command 'rsDriver()'. The url of the supermarket Mercadona is loaded into a variable called 'merc' which is then inserted into the 'remDr$client$navigate()' function, to browse the Mercadona website in the background browser.

```{r}

#Set the user agent
set_config(
  user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36; Nienke Visscher / nienvis@gmail.com")
)

#Opening hidden browser
remDr <- rsDriver(port = 4468L, browser = "firefox",  verbose=F, chromever = NULL)

#Loadingf the Mercadona url into the variable 'merc'
merc <-"https://tienda.mercadona.es/categories"
 
#Browsing to the website of Mercadona in the hidden browser
remDr$client$navigate(merc)


```

### Obtaining URLs

#### Clicking through opening page

First, the function 'remDr\$client' is summarised into 'driver'. The following chunk should only be executed when the port is newly initiated. Before one is allowed to browse the Mercadona website, the cookies need to be accepted and a postal code needs to be inserted. After each clickElement(), a system sleep of 1 second is included to avoid errors coming from the code running faster than selenium is able to browse.

```{r}

#Summarising the function'remDr$client' into 'driver'
driver <- remDr$client

Sys.sleep(3)

#Note following only when having newly initiated the port
#Accepting cookies
driver$findElement(value = "(//button[@class = 'ui-button ui-button--small ui-button--tertiary ui-button--positive'])[1]")$clickElement()

#Click on text box
driver$findElement(value = "(//input[@class = 'ym-hide-content'])[1]")$clickElement()

#Define findElement of the textbox
codigo <-driver$findElement(value = "(//input[@class = 'ym-hide-content'])[1]")

#Fill in the text box
codigo$sendKeysToElement(list("28014"))

#Click 
driver$findElement(value = "(//button[@class = 'button button-primary button-big'])[1]")$clickElement()

Sys.sleep(1)

```

#### Obtain the URLs per category

The two categories that will we scraped from the Mercadona website for this assignment are 'Cuidado del cabello' and 'Cuidado facial y corporal'. Due to the structuring on the website, HTML code for each subcategory within the two main categories only becomes visible when changing to the url of this specific subcategory. Therefore, the urls for each subcategory need to be extracted to be able to read the html code. To do so, one loop per main category is created. First, one needs to create an empty list to store the urls. Then, using the function 'driver\$findElement()' one navigates to the button of the first category 'Cuidado del cabello' and clicks on this button using the function 'clickElement()'. Now, the category 'Cuidado del cabello' as well as the first subcategory are entered and the url is extracted using the function 'driver\$getCurrentUrl()' and is stored in the url list. The category 'Cuidado del cabello' has three more subcategories left that need to be scraped. When one scrolls down to the end of the page of the subcategory, one finds a button which directs the user to the next sub category. Using a 'for loop', this button is clicked and the url of this next subcategory is saved in the url list. When the list includes 4 urls, the loop terminates. This method is repeated for the second category. Two seperate loops have to be initiated as the html code of the subcategories does not allow clicking on the side navigation bar to navigate to the next category. When all urls are collected in the url list, the function unlist() is utilised to convert to an unnested list.

```{r}

#Get URLs 'Cuidado del cabello'

#Create an empty list to store the URLs 
url <- list()

#Navigate to the button 'Cuidado del cabello'
driver$findElement(value = "(//label[@class='subhead1-r' and contains(text(), 'Cuidado del cabello')])[1]")$clickElement()

#Use system sleep to avoid code running faster than the browsing
Sys.sleep(1)

#Store the first URL
url[1] <-driver$getCurrentUrl()

#Create a loop to click on the button at the end of the page to naviagte to the next subcategory and save the url
for (i in (2:4)){
 
#Find the button at the end of the page and click to navigate to the next subcategory
  driver$findElement(value = "(//button[@class = 'ui-button ui-button--big ui-button--secondary ui-button--positive category-detail__next-subcategory'])[1]")$clickElement()
  
#Use system sleep to avoid code running faster than the browsing
Sys.sleep(1)
  
#Grab the url and store the url in the url list
url[i] <-driver$getCurrentUrl()
  
  
}

#Get URLs 'Cuidado facial y corporal'
  
driver$findElement(value = "(//label[@class='subhead1-r' and contains(text(), 'Cuidado facial y corporal')])[1]")$clickElement()

Sys.sleep(1)

url[5] <-driver$getCurrentUrl()



for (i in (6:15)){
  
  
  driver$findElement(value = "(//button[@class = 'ui-button ui-button--big ui-button--secondary ui-button--positive category-detail__next-subcategory'])[1]")$clickElement()
  
  Sys.sleep(1)
  
  url[i] <-driver$getCurrentUrl()
  
}

#Unlist the URL variable
url <-unlist(url)


```

### Scraping and cleaning data

In the following chunk, the html code for each subcategory is collected, the needed data is extracted, cleaned and consequently added to a data frame. First, the function navigates to one of the urls in the url list. A system sleep is ultilised to avoid the code running faster than the browsing which will cause errors. Then, the html code is collected using the function 'driver\$getPageSource()' and is read using the function read_html(). At this point, the html code is stored in the 'cat_html' variable and can be navigated using functions from the 'xml2' package. Hereafter, the data of the products is stored in the variable 'cat_info' by first finding the correct tag which stores the data with the function 'xml_find_all()', and consequently converting the tag to text using the function xml_text(). This method is repeated to collect the names of the subcategories. The xml text strings containing the data of the products is cleaned and separated into smaller strings. First the strings are separated by inserting a / in the places where the string needs to be split using the 'str_replace_all()' function and consequently splitting the string according to the / sign. The strings are splitted into a name, quantity, quantity unit and price. Then, these substrings are added to a data frame together with the subcategory strings. Finally, the function 'map_dfr()' is used to loop all urls in the url list over the function 'cat_grabber' resulting in a data frame containing data on all the products of the categories 'Cuidado del cabello' and 'Cuidado facial y corporal' of the supermarket chain Mercadona.

```{r}

#Create function
cat_grabber <- function(url){
  
#Navigate to one of the URLs stored in the variable URL
driver$navigate(url)
  
#Use system sleep to avoid code running faster than the browsing
Sys.sleep(1)

#Get the page source code and read the html
cat_html <-driver$getPageSource()[[1]] |>
  read_html()

#Collect data on products
cat_info <- cat_html|> 
    xml_find_all("//*[@id='root']//button/div[2]") |>
    xml_text()

#Collect the sub category names
subcat_name <- cat_html |> 
   xml_find_all("//h1[@class='category-detail__title title1-b']") |>
   xml_text()

#Clean and separate data using regex
cat_clean<-cat_info|> 
#Replace all €, /, . or ! signs with an empty space
  str_replace_all("\\€ |/|\\.|!", "") |> 
#Replace all € signs followed by 1 or more digits, followed by a comma and again   by one or more digits with an empty space
  str_replace_all("\\€\\d+,\\d+", "") |>
#Replace ml with /ml/ when looking backwards finding first an empty space and then a digit
  str_replace_all("(?<=\\d\\s)ml", "/ml/") |> 
#Replace ud with /ud/ when looking backwards finding first an empty space  followed by a digit
  str_replace_all("(?<=\\d\\s)ud", "/ud/") |> 
#Replace ud with /ud/ when looking backwards finding first 0,1 or 2 empty spaces followed by a digit
  str_replace_all("(?<=\\d\\s{0,2})ud", "/ud/") |> 
#Replace L with /L/ when looking backwards finding first an empty space followed by a digit
  str_replace_all("(?<=\\d\\s)L", "/L/") |> 
#Replace 'tarro' with an empty space when looking backwards finding first an empty space followed by a digit
  str_replace_all("(?<=\\d\\s)tarro", "") |> 
#Replace g followed by an empty space with /g/ when looking backwards finding first an empty space followed by a digit
  str_replace_all("(?<=\\d\\s)g\\s", "/g/") |> 
#Replace g with /g/ when looking backwards finding first an empty space followed by a digit
  str_replace_all("(?<=\\d\\s)g", "/g/") |> 
#Replace 'sobres' with /sobres/ when looking backwards finding first an empty space followed by a digit
  str_replace_all("(?<=\\d\\s)sobres", "/sobres/") |> 
#Replace 'bandas' with /bandas/ when looking backwards finding first an empty space followed by a digit
  str_replace_all("(?<=\\d\\s)bandas", "/bandas/") |> 
#Replace '1' with / when looking backwards finding first an empty space followed by the word DovePaquete
  str_replace_all("(?<=sensibleCaja\\s)1", "/") |>
#Replace '4' with / when looking backwards finding first an empty space followed by the word DovePaquete
  str_replace_all("(?<=DovePaquete\\s)4", "/") |> 
#Replace ud. with an empty space when looking foreward finding 0 or more empty spaces followed by ()
  str_replace_all("ud.(?=\\s*\\()", "") |> 
#Replace an empty space with an / when looking foreward finding 1 or more digits followed by 0 or more empty spaces and a / 
  str_replace_all("\\s(?=\\d+\\s*/)", "/") |> 
#Replace a lower case letter followed by a digit with /
  str_replace_all("(?<=[[:lower:]])(?=[[:digit:]])", "/") |> 
#Replace a literal ( or a literal ) with an empty space
  str_replace_all("\\(|\\)", "") |> 
#Replace all literal . with an empty space
  str_replace_all("\\.", "")|> 
#Split the strings by /
  strsplit("/")


#Only keep the variables with 5 or less strings
cat_clean<- cat_clean[lengths(cat_clean) == 5]

#Create a dataframe
data.frame(
  name = sapply(cat_clean, `[`, 1),
  quantity = sapply(cat_clean, `[`, 2),
  quantity_unit = sapply(cat_clean, `[`, 3),
  price = sapply(cat_clean, `[`, 4),
  ud =  sapply(cat_clean, `[`, 5),
  subcat =  sapply(subcat_name, `[`, 1),
  stringsAsFactors = FALSE
)


}

cat_grabber(url)

#Loop thhe function cat_grabber over all URLs
final <- map_dfr(url, cat_grabber) 

```

#### Scrape and add main categories to data frame

To scrape and add the main categories to the data frame, the function 'driver\$navigate()' is used to navigate to first url in the list which is the home page including all main categories. Then, the html code is collected and read using the functions 'driver\$getPageSource()' and 'read_html()'. The function 'xml_find_all()' is used to find the correct tag followed by the use of the xml_text() function in order to convert the tag into text which is stored in the variable 'cat'. The category 'Cuidado del cabello' and 'Cuidado facial y corporal' are stored in the 14th and 15th position of the list respectively. Consequently, the categories are added to the data frame by matching to their subcategories.

```{r}

#Navigate to the first url
driver$navigate(url[1])
  
Sys.sleep(1)

#Grab the html code and read the html to make it accessible to functions from the 'XML2' package
cat_html <-driver$getPageSource()[[1]] |>
  read_html() 

#Collect the category names 
cat <- cat_html |> 
   xml_find_all("//label[@class='subhead1-r']") |>
   xml_text()

#Add the category names to the data frame
final <- final |> 
  mutate(cat_name = ifelse((subcat=="Acondicionador y mascarilla"|
                              subcat== "Champú"|
                              subcat== "Coloración cabello"|
                              subcat== "Fijación cabello" ),cat[14], cat[15]))

```

```{r}
#Deslect column ud bacause this column does not contain variation
final <- final |> 
  dplyr::select(-ud)
```

### Extracting brand names

In the chunk below, the brand names are extracted. The brand names are included into the name of the product without a clear or reoccurring structure. Therefore, no regex could be applied to extract these brand names. For this reason, the different brand names in the data are identified by hand and collected into a vector. Then, this vector is collapsed into a string called 'brands_or', separating the brand names by an \| sign. This string is used to extract the brand names from the product names using the function 'str_extract_all()' and these extractions are added to a new column called 'brand'.

```{r}

brands <- c("Deliplus", "Elvive", "Pantene", "O'lysee", 
            "H&S", "Ultrex", "Garnier", "L'Oréal", 
            "Schwarzkopf", "Nelly", "Giorgi", "Elnett", 
            "Fructis", "Syoss", "Gillette", 
            "Nivea", "Axe", "Atlantia", "Bosque Verde", 
            "Montagne Jeunesse", "Viseger Pharma", 
            "My Moment", "Khanya", "Veet", "Wilkinson", 
            "Sanex", "Byly", "Tulipán Negro", "Dove", 
            "Deonat", "Rexona", "Heno de Pravia", "960", 
            "La Toja", "Magno", "Lactovit", "Colgate", 
            "Oral-B", "Sensodyne", "Signal", "Parodontax", 
            "Benfix", "Listerine", "Polident", "evax", 
            "Ausonia", "Carefree", "Tampax", "Tena",
            "Sense", "Classic fresh", "Rose Nude", 
            "Elección", "Prêt à Porter", "Como Tú", 
            "Como Tú", "Psicodelic", "Complicity", "Vuela", 
            "Adidas", "Soplo", "Capítulo Floral", "My soul", 
            "Blue Shine", "Verissime", "Flor de Mayo", 
            "Monogotas", "Capítulo", "Extrait", 
            "Mr. Wonderful", "Uahuu", "TokiDoki", "Ikiru",
            "Chupachups", "Rose Nude", "Rouge seduction",
            "Snoopy", "Muy mío", "Impacto", "Afán", 
            "Springfield", "Gesto", "Sesgo", "Brummel", 
            "Antonio Banderas", "Misty Wood", "Capítulo Marino", "Ego", 
            "My Soul", "Jacq's", "My Soul", "Selk",
            " Crossmen", "Gesto", "Disney", "Marvel", 
            "Naruto", "Peppa Pig", "Sonic de Hedgehog", "Colour up!", 
            "HoneyBotella")


brands_or <- paste(brands, collapse = "|")

final <- final %>% 
  mutate(brand = str_extract(name, regex(brands_or, ignore_case = TRUE)))
```

### Standardising prices

As the prices of the products are for different quantities which are measured in different units, the prices of the products need to be standardised for them to be comparable. Hence, the price of liquid products are converted to the price of this product per 100 ml, the prices of products measured in grams are converted to the price per 100 grams, and the price of products measured in units are converted to the price of 1 unit. The latter is done using the function 'case_when()' and applying the correct converting formula to the price, based on the detected measurement unit using the function 'str_detect()'.

```{r}
units <- c("L", "ml", "ud", "sobres", "bandas", "g")

mercadona <- final |> 
  filter(quantity_unit %in% units) |> 
  filter(str_detect(price, "[0-9]")) |> 
  mutate(price = str_replace_all(price, ",", ".")) |> 
  mutate(quantity = as.numeric(quantity),
         price = as.numeric(price)) |> 
  mutate(avg_price = case_when(
    str_detect(quantity_unit, "g|ml") ~ (price/quantity)*100,
    str_detect(quantity_unit, "L") ~ (price/quantity)/100,
    str_detect(quantity_unit, "ud") ~ price /quantity
    )) |> 
   mutate(avg_price = round(avg_price, 2))

```

### Assigning marketed gender to product

In the following chunk, the targeted gender of the product is extracted and added to a new column in the data frame. First, different words that indicate gender are added to a vector called 'gender' which is then collapsed into a string separating the words by an \| sign. As both the name of the product as well as the subcategory name can contain information on the gender, these two columns are temporarily combined into the variable 'combine'. Then, the function 'str_extract()' is used to extract the gender names from the 'combined' variable and the names are stored in the variable 'gender_name'. When no gender is detected, the products is assigned NA which are then renamed as 'non-male'. Hereafter, two separate vectors inlcuding male and female words are created. These vectors are used to assign 'Male', 'Female' or 'Non-male' to the products through applying the function case_when() and is stored in the variable 'gender'. Then, this process is repeated again but this time only assigning 'male' or 'non-male' to products.

```{r}
#Vector of words indicating gender
gender <-c("Men", "Women", "Man", "Woman", "Hombre", "Mujer", "Hombres", "Mujeres", "Him", "Her", "Niña", "Niñas", "Homme", "Femme", "Male", "Female", "Males", "Females", "Venus") 

#Collapsing the gender vector to a string 
gender_or <- paste(gender, collapse = "|")

mercadona <-mercadona |> 
  mutate(combined =  paste(mercadona$name, mercadona$subcat, sep = " ")) |> 
  mutate(gender_name =  str_extract(combined, regex(paste0("\\b(", gender_or, ")\\b"), ignore_case = TRUE)) ) |> 
  select(-combined)

# Convert NA to 'Non-Male'
mercadona$gender_name[is.na(mercadona$gender_name)] <- "Non-Male"


#Create Male and Female vectors
male <- c("Men", "Man", "Hombre", "Hombres", "Him", "Homme", "Male",  "Males") 

male <- c(male, tolower(male))

female <- c("Women", "Woman", "Mujer", "Mujeres", "Her", "Niña", "Niñas", "Femme", "Female", "Females", "Venus") 

female <- c(female, tolower(female))

#Assign gender to products (separate column for female)
mercadona$gender <- case_when(
  mercadona$gender_name %in% male | mercadona$subcat %in% male ~ "Male",
  mercadona$gender_name %in% female|mercadona$subcat %in% female ~ "Female",
  mercadona$gender_name == "Non-Male"~ "Non-Male")

#Assign male or non-male to products
mercadona$gender_2 <- case_when(
  mercadona$gender == "Male" ~ "Male",
  mercadona$gender == "Female" ~ "Non-Male",
  mercadona$gender == "Non-Male"~ "Non-Male")

remDr$server$stop()
```

## Clarel

For Clarel website, there is a lot of content hidden through Javascript so we will use Selenium to scrape all the interested content. The setup on this computer was initially troublesome as we wanted to use Chrome as browser. There were a few fix suggestions and the only one that worked was to delete the "LISCENSE.chromever" file for the corresponding. However, there was no clear explanation to this fix and the potential issues with deleting the file so we will use Firefox browser for selenium instead.

### Data Collection

```{r}
driver <- rsDriver(browser = "firefox",
                   chromever = NULL,
                   verbose = F)

remDr <- driver[["client"]]
```

We first initialize the Selenium server and create a remote driver session. We're saving the client object to a variable for quicker access later on.

#### Main Webpage

```{r}
clarel_url <- "https://www.clarel.es/es/"

remDr$navigate(clarel_url)

Sys.sleep(20)

#Reject cookies
remDr$findElement(using = "xpath", "//button[contains(@class, 'info_cookie-consent-reject-button')]")$clickElement()
```

Once we navigate to the home page of Clarel, there is a pop up for cookies so we will need to either accept or reject the cookies. We leave the system to pause for 5 seconds after navigating to the home page so the browser has sufficient time to load everything. If reject cookies line code runs into error, most likely it is because the page has not fully loaded and the pop up has not appeared yet therefore the button is not found. It is necessary to run again the reject cookies code. If it still runs into error, it is likely that the elements in the web page have changed their name.

#### Scrape Hombre Category

We will start by scraping one category before streamline the scraping process for multiple categories. The structure of the pages of all categories are quite similar so we should not run into many troubles later on if we can scrape one category properly. In this project, we chose to start with "Hombre" category.

```{r}
#helper function to get all subcategories of main category input
get_subcat2_url <- function(category){
  #get all the nodes containing category keyword
  all_nodes <- remDr$getPageSource()[[1]] %>% 
    read_html() %>% 
    xml_find_all(glue("//a[contains(@href,'/{category}/')]")) %>% 
    xml_attr("href") %>% 
    .[str_detect(.,"https",negate = TRUE)] %>% 
    unique(.)
  
  #get subcategories and sub-subcateogry(product type) 
  sub_cat <- all_nodes[str_detect(all_nodes, glue("es/{category}/\\w+(-\\w+)?$"))]
  product_type <- all_nodes[str_detect(all_nodes, glue("{category}/\\w+(-\\w+)?/\\w+"))]
  
  #identify subcategories with no product type to include in list to scrape
  no_product_type <- c()
  for (i in sub_cat) {
    if(!any(str_detect(product_type, i))) {
      no_product_type <- c(no_product_type,i)
    }
  }
  
  #all urls to subcategories and sub-subcategories of the main categories
  all_urls <- c(no_product_type, product_type) %>% 
    paste0("https://www.clarel.es",.)
  
  return(all_urls)
}

#test run
sub_cat_2_url <- get_subcat2_url("hombre")
sub_cat_2_url
```

In each main category of products on Clarel, there are also sub-categories and in each of the sub-category sometimes there are sub-section (they will be referred as product type in this document) with the details the product type as well. We would like to get all of this information so later we have more resources to work with for the analysis in case that we need it.

To do this, first a helper function is created to grab all the URLs to the sub-categories and product types. Another option would have been to navigate to each sub-category and grab the product types but it is more time consuming as we would need to go to multiple product type pages of multiple sub-category pages and the loading time would amass quickly. Therefore, we tried to identify all the URLs from the home page but the positive trade-off is some cleaning in the process.

The logic of this function is of below:

-   The input (parameter) is the main category that we are interested in.

-   The function then finds all the anchor links containing the category, extracts those links, cleans irrelevant links and duplicated links, then saves them into a variable.

-   The function continues to extract sub-category links and product type links. The product types are subsection of the sub-category, therefore, we need to exclude them from the URL list to avoid scraping duplicated items.

-   The cleaning of these links are done by detecting if the URL list of product type contains the sub-category. (For example, "afeitado" sub-category URL contains "es/hombre/afeitado". A product type of this sub-category is "despues afeitado" and its URL would be "es/hombre/afeitado/despeues-afeitado". The URL of "despues afeitado" contains URL of afeitado so we can base on that and exclude "afeitado" from the list)

-   The function returns a vector of product type URLs and sub-categories with no product types URLs

Using this function, we can obtain all the necessary URLs to scrape all the products of the main category divided by sub-categories and product types. Now we will move on to scrape the products of each URL.

```{r}
#Load more items helper function
load_all_items <- function() {
  Sys.sleep(2)
  tryCatch(
    {
      suppressMessages({
      load_more_button <-remDr$findElement(using = "xpath", 
                                          "//button[contains(@class, 'load-more-items-button')]")
      while(load_more_button$isElementDisplayed()[[1]]) {
      load_more_button$clickElement()
      Sys.sleep(2)
      load_more_button <-remDr$findElement(using = "xpath", 
                                           "//button[contains(@class, 'load-more-items-button')]")
      }
      })
    },
    error = function(e){})
}

#test run
# load_all_items()
```

Navigating to the URL of the sub-categories and product types gives us a list of products. However, pages have a lot of products and not all are shown on the page. These products are hidden after the "Ver más productos" ("See more products") and we need to repeatedly click on the button until all the products are shown.

Above, we created a function to click the button until the button is gone, meaning that all products have been loaded. There are pauses between each click to wait for the products to load. Since we do not know how many times we have to click the button, we use a `while` loop to click the button until there is no more button to click. When there is no more button, we would get an error if we tried to find the button so we use a `tryCatch()` function to catch and ignore this error.

With this helper function to load all the products, we now can scale and go to all URL, load all the products, and scrape them.

```{r}
#Get all product info helper function
get_product_info <- function(url) {
  remDr$navigate(url)
  
  load_all_items()
  
  paths <- read_html(remDr$getPageSource()[[1]]) %>% 
  xml_find_all("//div[contains(@class, 'box_product-info')]")
  
  get_info <- function(path) {
  brand <- path %>% 
    xml_find_all("div[contains(@class, 'product-featuredbrands')]") %>%
    xml_text()
  
  product_name <- path %>% 
    xml_find_all("a") %>% 
    xml_text()
  
  price <- path %>% 
    xml_find_all("div[contains(@class, 'product-price')]/span[contains(@class,'final-price')]") %>% 
    xml_text()
  
  data.frame(
    brand = brand,
    product_name = product_name,
    price = price
  )
  }
  
  #Get categories & subcategories to add to the dataframe
  cat <- remDr$getPageSource()[[1]] %>% 
    read_html() %>% 
    xml_find_all("//span[@class='subcategory']") %>% 
    xml_text() %>% 
    paste(collapse = "-")
  
  df <- map_dfr(paths, get_info) %>% mutate(full_cat = cat)

  
  return(df)
}

#test run
hombre_products <- map_dfr(sub_cat_2_url, get_product_info)

hombre_products
```

Now, we need to get the information of the products. We will similarly create a helper function to streamline the process. The function we created take a URL for input, navigates to that URL, loads all the products using the function created previously, then it gets all the paths (nodes) of the `div` tags containing information about the products. After this, we nested a function to grab the product information from the `div` tags, which at the end returns the data frame of the product information, including brand name, product name, and price. Since categories, sub-categories, and product types are in other elements of the page, we grabbed them separately and append to the data frame to finally create a data frame with brand, product name, price, and full category columns consisting of main category, sub-category, and product type (if any) joined together by "-" (e.g., "Hombre-Afeitado-Después Afeitado").

#### Scrape Multiple Categories

```{r}
#all main categories to be scraped
imp_cat <- c("hombre", "higiene", "cabello", "perfumes")

#helper function to scrape data on large scale
scrape_all <- function(category) {
  remDr$navigate(clarel_url)
  
  subcat2 <- get_subcat2_url(category)
  
  df <- map_dfr(subcat2, get_product_info) 
  
  return(df)
}

all_products_clarel <- map_dfr(imp_cat ,scrape_all)

#write into csv to avoid scraping every time
write_csv(all_products_clarel, "datasets/clarel.csv")

#close server
driver$server$close()
```

With the helper functions created before, we have all the tools needed to scrape products from Clarel web page on a large scale (multiple categories). We create a vector with all the interested categories and a function to automate the scraping. This function navigates to the home page of Clarel, then extracts all URLs of sub-categories and product types of the main category input, and finally gets all the product information using the function from the last section to produce a final data frame. We use `map_dfr` to apply the function on the vector of interested categories and create a comprehensive data frame.

At the time of writing this project, it took around 8 minutes to scrape the 4 categories hombre, higiene, cabello, and perfumes. We saved the scraped data into a csv file so for the analysis we would not have to repeat the scrape every time. Furthermore, if there are any changes to the web page and the functions no longer work, the analysis is still reproducible with the csv file.

### Data Cleaning

```{r}
#loading the data frame from the file created after scraping
all_products_clarel <- read_csv("datasets/clarel.csv")
```

```{r}
#helper function to id gender & kid of products
gender_id <- function(str){
  male <- c("men", "man", "hombre", "hombres", "him", "homme", "male",  "males", "masculino") 

  female <- c("women", "woman", "mujer", "mujeres", "her", "femme", "female", "females", "venus", "femenina") 

  kids <- c("kid", "kids", "niños", "niño", "niñas", "niña", "infantil", "infantiles")
  
  str <- tolower(str)
  
  if(str_detect(str, paste0("\\b(", kids, ")\\b", collapse = "|"))) {
    return("Kid")
  } else if(str_detect(str, paste0("\\b(", female, ")\\b", collapse = "|"))) {
    return("Female")
  } else if(str_detect(str, paste0("\\b(", male, ")\\b", collapse = "|"))) {
    return("Male")
  } else {return(NA)}
  
}
```

It is extremely tricky to identify which products are aimed at men or women (especially women) because most products do not include this information. They are more easily identified through the pictures and packaging choice (such as pink color and with a woman figure for women products) but since we work with only text we are very limited to the ability to identify to which gender the product is geared toward. Some products are in "Hombre" category so they are easily classified for men but the other categories do not clearly indicate gender. We will try to detect this information using keywords in the product names and sub-categories.

Above, we created some lists of words that could indicate whether the products are for men or women or kids. We also try to identify kids products because later on we will discard them since it is not the scope of this project. We then put those lists into a function that takes a string as input and detects whether in the string there is any word from the list, then categorizes the string accordingly.

```{r}
cleaned_clarel <- all_products_clarel %>% 
  as_tibble() %>% 
  #Remove duplicated products
  distinct(product_name, .keep_all = TRUE) %>% 
  #Mutate to later split into cateogries and subcategories
  mutate(
    full_cat = str_replace(full_cat, "-", "/")
  ) %>% 
  #Split category into sub category and product type
  separate_wider_delim(full_cat, delim = "/", names = c("category", "sub_category")) %>% 
  #Extract quantity with unit
  mutate(qty = str_extract(tolower(product_name), "\\d+ ?ml|\\d+ ?g|\\d+ ?ud(s)?|\\d+(,\\d+)? ?l") ,.after = product_name) %>% 
  #NA rows in quantity means product is sold in 1 unit
  mutate(qty = replace_na(qty, "1")) %>% 
  #Extract unit of quantity
  mutate(
    qty_unit = ifelse(qty == 1, "ud", str_extract(qty, "ml|l|ud|g")),
    price = parse_number(str_replace(price, "€", ""), locale = locale(decimal_mark = ",")),
    qty = parse_number(qty,locale = locale(decimal_mark = ","))) %>% 
  #Calculate average price
  mutate(
    avg_price = case_when(
      str_detect(qty_unit, "ml|g") ~ price * 100 / qty, #price per 100ml/g
      qty_unit == "ud" ~ price / qty, #price per unit
      qty_unit == "l" ~ price *0.1 / qty, #price per 100ml for Litters
      TRUE ~ price 
    )
  ) %>% 
  #Round average price and identify targeted gender of product
  mutate(avg_price = round(avg_price, 2),
         gender = case_when(
           (category == "Hombre" | str_detect(sub_category, "Hombre")) ~ "Male",
           TRUE ~ mapply(gender_id, paste(product_name, sub_category))
           )
         ) %>% 
  #Replace non gender specific product with non-male
  mutate(gender = replace_na(gender, "Non-Male"),
         #gender_2 to compare male vs non-male targeted products
         gender_2 = ifelse(gender == "Male", "Male", "Non-Male")) %>% 
  #Remove products for kids
  filter(gender != "Kid") %>% 
  #Reorganise column names to join with other datasets
  dplyr::select(product_name, qty, qty_unit, price, sub_category, category, brand, avg_price, gender, gender_2)
```

As for most web scraping projects, the scraped data is usually messy and needs diligent cleaning so it is appropriate for analysis. The same goes for the data scraped here previously.

First we remove the duplicated products since products in some categories overlap and therefore appear multiple times in the data set. We then split the full category into main category and sub-category (which also includes product types). We first replace the first "-" of the full category collumn with another symbol so we can split into 2 columns based on that symbol.

Next, we extract the quantity from the product name. The product name contains various information from the product to brand name, quantity, and more. We extract the quantity using Regex and look for numbers followed by quantity symbol (ml, L, g, u/ud/uds). If the product name does not contain any of these, it is most possible that the product is sold as one unit whole. Therefore, after extracting products with specific quantities, we impute the missing rows with 1 unit.

We then extract the unit of the quantity then calculate the average price based on the unit. For milliliter (ml), gram (g), and litter (l) the average price is per 100 ml or g. For products that are sold in more than 1 unit the average price is calculated as per unit. Lastly, they are rounded at 2 decimal places.

Finally, we detect the gender for which the product aims. First, all products in category "Hombre" are named "Male" and then we use the gender identifying function from before to detect if the product name or product sub-category/product type indicates that it is for men or women or kids. Since few products are explicitly described as for women, we make another label called "Non-Male" to compare specifically male-targeted products versus non-male-targeted products. The gender column include 4 different labels (Male, Female, Kid, and Non-Male). The gender_2 column consists of only 2 (Male and Non-Male). These are made for the purpose of analysis.

## Data Joining

```{r}
#Modify mercadona dataframe to join
mercadona_cleaned <- mercadona |> 
  select(-gender_name) |> 
  mutate(store = "Mercadona")

cleaned_clarel <- cleaned_clarel |> 
  mutate(store = "Clarel")

#Match column names
colnames(cleaned_clarel) <- colnames(mercadona_cleaned)

#helper function
match_cat <- function(str) {
  str <- tolower(str)
  
  cat <- case_when(
    str_detect(str, "afeitado") ~ "Afeitado",
    str_detect(str, "champú") ~ "Champú",
    str_detect(str, "acondicionador|mascarilla") ~ "Acondicionador y Mascarilla",
    str_detect(str, "coloración|tinte") ~ "Coloración",
    str_detect(str, "fija") ~ "Fijación cabello",
    str_detect(str, "corporal") ~ "Cuidado corporal",
    str_detect(str, "facial") ~ "Cuidado facial",
    str_detect(str, "depilación") ~ "Depilación",
    str_detect(str, "desodorante") ~ "Desodorante",
    str_detect(str, "gel") ~ "Gel y Jabón",
    str_detect(str, "higiene bucal") ~ "Higiene bucal",
    str_detect(str, "higiene íntima") ~ "Higiene íntima",
    str_detect(str, "manicura|pedicura") ~ "Manicura y Pedicura",
    str_detect(str, "perfume") ~ "Perfume",
    str_detect(str, "solar") ~ "Protector solar y aftersun"
  )
  
  return(cat)
}

#Join 2 dataset together

big_df <- bind_rows(mercadona_cleaned, cleaned_clarel) %>% 
  #sub-category matching
  mutate(match_subcat = match_cat(subcat), .after = subcat) %>% 
  #match by product name if sub-cat doesn't match
  mutate(match_subcat = ifelse(is.na(match_subcat), match_cat(name), match_subcat)) %>% 
  #other subcats that have no match for either sub-cat and product name
  mutate(match_subcat = replace_na(match_subcat, "Other"))

```

```{r}
big_df <- big_df |> 
  mutate(type_brand = ifelse((brand == "Deliplus"|brand == "NESK"|brand == "BONTE") ,"Housebrand", "Brand"))
```

### Saving the data

Given that the html code of the websites might change which will cause the scraping code to stop functioning, the full data frame is stored in a csv file names 'full_data.csv'. Then, the csv file is loaded back into R to continue with the analysis.

```{r}
write_csv(big_df, "full_data.csv")

full_data <- read_csv("full_data.csv")

df_balanced <- full_data
```

## Data balancing

Given that the data set is rather unbalanced with regards to the number of 'male' and 'non-male' products, the data set is balanced based on the 'gender_2' variable. The balancing method applied both undersamples the majority class and oversamples the minority class to generate a data set in which approximately half of the cases are from the original majority class and half of the cases of the original minority class. This balancing is necessary for the application of t-test to compare the price means of the 'male' and 'non-male' category.

```{r}
full_data |> 
  ggplot(aes(x=gender_2)) +
  geom_bar()

full_data <-full_data |> 
  drop_na()

table(full_data$gender_2)
prop.table(table(full_data$gender_2))

df_balanced <- ovun.sample(gender_2~., data=full_data, method = "both",
                    p = 0.5,
                    seed = 123,
                    N = 3600)$data #double of the train set
table(df_balanced$gender_2)

df_balanced |> 
  ggplot(aes(x=gender_2)) +
  geom_bar()


```

## Mean comparisons

In this section, the prices between male and non-male products are compared with each category and subcategory.

### Categories

To start, a function is created within which first a data set is created named 'cat_d' which is the 'df_balanced' but filtered by a specific combination of the category name and the quantity unit. Then, the standardised price ('avg_price') is filtered by 'male' and 'non-male' and stored in the variables 'prices_male' and 'prices_non-male' respectively. Hereafter, an empty variable named 't_test_result' is created to later store the results from the t-test analyses and the t-test function is inserted within a 'tryCatch()' function as not all t-tests will be valid and generate results. When the t-test failed, the results are assigned NA and the function continues. The t-test function tests whether the prices of the male products are significantly lower then the prices of non-male products. The function 'tidy()' is used to convert the output of the t-test into a tibble.

Once the function is correctly specified, a loop function is created to apply the function to all combinations of category and quantity unit. The unique category names are saved into a data frame named 'cat_loop' using the function 'dinstinct()'. Then, the column 'cat_name' from the 'cat_loop' data frame is converted into a nested list by applying the 'as.list()' function and is then unnested through the function 'unlist()'. As each category will be tested per quantity unit, a new list names 'cat_f' is created in which the category names are duplicated using the 'rep()' function. A vector called 'quan_f' is created in which the two main quantity units 'ml' and 'ud' are alternating repeated both 6 times. An empty list named 'x' is created to store the results of the t-test. Hereafter, a loop is created which applies the 't-test-fun' to the items in the 'cat_f' and 'quan_f' vectors. All tibbles containing the results of the different t-test are combined into one data frame named 'result_cat' using the function 'do.call()' in combination with 'rbind()'. The corresponding categories and units are added to the 'result_cat' data frame to enhance interpretability. Then, this data frame is cleaned by properly renaming the columns and organising the columns in a more logical order.

```{r}

cat_f <- c("Higiene")
quan_f <- c("ml")


t_test_fun <- function(cat_f, quan_f) {
  cat_d <- df_balanced |> 
    filter(cat_name == cat_f & quantity_unit == quan_f)
  
  prices_male <- cat_d$avg_price[cat_d$gender_2 == "Male"]
  prices_non_male <- cat_d$avg_price[cat_d$gender_2 == "Non-Male"]
  
  t_test_result <- NULL
  
  tryCatch({
    t_test_result <- tidy(t.test(prices_male, prices_non_male))
  }, error = function(e) {NA
    
  })
}

t_test_fun(cat_f, quan_f)

cat_loop<- df_balanced |> 
  distinct(cat_name)

cat_loop <- as.list(cat_loop$cat_name)
cat_loop <- unlist(cat_loop)
cat_f <- rep(cat_loop, each = 2) 


quan_f <- c("ml", "ud")
quan_f <- rep(quan_f, times = 6)


x <- list()

for (i in seq_along(cat_f)) {
  result <- t_test_fun(cat_f[i], quan_f[i])
  x[[i]] <- result
}

# Combining the results into a data frame
result_cat <- do.call(rbind, x)
result_cat$category <- cat_f
result_cat$quantity_unit <- quan_f

result<-result_cat |> 
  select(category, quantity_unit, everything()) |> 
  rename(diff_male_non_male = estimate, 
         male = estimate1, 
         non_male = estimate2,
         t_statistic = statistic,
         df = parameter) |> 
  mutate(p.value = round(p.value, 3)) |>  
  drop_na() 

resul_sig <- result |> 
  filter(p.value <=0.05)
  



```

### Sub-categories

The method for obtaining the t-test results is repeated with minor adjustments to obtain the t-test results for the sub-categories. The data is filtered based on the subcategory. Also, the number of repetitions of the quantity units are changed according to the number of subcategories

```{r}
cat_f <- c("Champú")
quan_f <- c("ml")


t_test_fun <- function(cat_f, quan_f) {
  cat_d <- df_balanced |> 
    filter(match_subcat == cat_f & quantity_unit == quan_f)
  
  prices_male <- cat_d$avg_price[cat_d$gender_2 == "Male"]
  prices_non_male <- cat_d$avg_price[cat_d$gender_2 == "Non-Male"]
  
  t_test_result <- NULL
  
  tryCatch({
    t_test_result <- tidy(t.test(prices_male, prices_non_male))
  }, error = function(e) {NA
    
  })
}

t_test_fun(cat_f, quan_f)

cat_loop<- df_balanced |> 
  distinct(match_subcat)

cat_loop <- as.list(cat_loop$match_subcat)
cat_loop <- unlist(cat_loop)
cat_f <- rep(cat_loop, each = 2) 


quan_f <- c("ml", "ud")
quan_f <- rep(quan_f, times = 16)


x <- list()

for (i in seq_along(cat_f)) {
  result <- t_test_fun(cat_f[i], quan_f[i])
  x[[i]] <- result
}

# Combining the results into a data frame
result_df <- do.call(rbind, x)
result_df$match_subcat <- cat_f
result_df$quantity_unit <- quan_f




result<-result_df |> 
  select(match_subcat, quantity_unit, everything()) |> 
   rename(diff_male_non_male = estimate, 
         male = estimate1, 
         non_male = estimate2,
         t_statistic = statistic,
         df = parameter) |> 
  mutate(p.value = round(p.value, 3)) |> 
  drop_na() 

resul_sig <- result |> 
  filter(p.value <=0.05)
```

## Discussion and conclusion

[Discussion results mean comparison]

### Limitations

As mentioned in the introduction, the analysis performed above could be greatly improved whne the detection of gender is done using image recognition methods rather than assigning by gender based on the name or descrtiption of the product as the latter often only contain information on gender in the case of male marketed products. However, the design of the product often clearly conveys whether the product is targeted at men or women trough for example the use of colour. Moreover, attributes that contribute to price differences such as the quality or whether the product is a brand or a house brand has not been included in this analysis. It is possible that the distribution of attributes is different for male and female products which influences the mean price of these products. Furthermore, it is advisable to include a larger number of stores to avoid the impact of store specific differences.

## Bibliography

Economic Committee United States Congress (2016). Retrieved March 11, 2024, <https://www.jec.senate.gov/public/_cache/files/8a42df04-8b6d-4949-b20b-6f40a326db9e/the-pink-tax---how-gender-based-pricing-hurts-women-s-buying-power.pdf>

Guittar, S. G., Grauerholz, L., Kidder, E. N., Daye, S. D., & McLaughlin, M. (2022). Beyond the pink tax: gender-based pricing and differentiation of personal care products. *Gender Issues*, *39*(1), 1-23.
